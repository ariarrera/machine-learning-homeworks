{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello!\n",
    "\n",
    "In this little piece of work I am goin to predict the sentiment of two channels, CNN and BBC, and see which one of those two produce the most positive news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "First of all, I am going to train two classifiers on Amazon data and see which one performs better, based on F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "amc = pd.read_csv('amazon_cells_labelled.txt', header=None, sep = '\\t')\n",
    "\n",
    "amc0 = amc.rename(columns={amc.columns[0]: \"text\", amc.columns[1]: \"score\"})\n",
    "amc0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good case , excellent value .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great for the jawbone .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the mic is great .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  so there is no way for me to plug it in here i...      0\n",
       "1                      good case , excellent value .      1\n",
       "2                            great for the jawbone .      1\n",
       "3  tied to charger for conversations lasting more...      0\n",
       "4                                 the mic is great .      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning and tokenizing the data\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "\n",
    "def clear_data(df):\n",
    "    df['text'] = df['text'].apply(lambda text: text.strip().split('|')[-1].split('http')[0].strip().lower())\n",
    "    df['text'] = df['text'].apply(lambda text: re.sub('[\\?!:)(«»@#$_1234567890#—ツ►๑۩۞۩•*”˜˜”*°°*`&]', '', text))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_and_stem(df):\n",
    "    df['text'] = df['text'].apply(lambda text: word_tokenize(text))\n",
    "    df['text'] = df['text'].apply(lambda text: [morph.parse(word)[0].normal_form for word in text])\n",
    "    df['text'] = df['text'].apply(lambda text: ' '.join(text))\n",
    "    return df\n",
    "\n",
    "\n",
    "amc_clean0 = clear_data(amc0)\n",
    "amc_clean = tokenize_and_stem(amc_clean0)\n",
    "amc_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cnn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents number :  1000 feature number:  1693\n"
     ]
    }
   ],
   "source": [
    "#creating a vector of words out of cleaned and toknized data\n",
    "words = []\n",
    "\n",
    "\n",
    "for line in open('stopwords-en.txt'):\n",
    "    line = line.split('\\n') \n",
    "    line = line[0]\n",
    "    words.append(line)\n",
    "\n",
    "words = frozenset(words)\n",
    "\n",
    "print('Vectorization of documents')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1,1), stop_words=(words))\n",
    "\n",
    "yx = vec.fit_transform(amc_clean['text'])\n",
    "amc_data = vec.fit_transform(amc_clean['text']).toarray()\n",
    "\n",
    "num_docs, num_feature = yx.shape\n",
    "print('documents number : ', num_docs, 'feature number: ', num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test collection size:  (330, 1693)\n",
      "Training collection size:  (670, 1693)\n"
     ]
    }
   ],
   "source": [
    "#creating a vector of answers\n",
    "Y = amc_clean['score'].values\n",
    "YY = Y.ravel()\n",
    "\n",
    "#here, I split the data on tarining and test sets with a threshold of 0.33\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(amc_data, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Test collection size: ', X_test.shape)\n",
    "print('Training collection size: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average value f1_macro (5 runs): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7715423480506086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# define the parameters for the model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=50, # The number of trees in the forest.\n",
    "    criterion='gini', #  function to measure the quality of a split (Gini impurity and “entropy”)\n",
    "    max_depth=5, # The maximum depth of the tree\n",
    "    min_samples_split=2, # The minimum number of samples required to split\n",
    "    min_samples_leaf=1, # The minimum number of samples required to be at a leaf node\n",
    "    min_weight_fraction_leaf=0.0, # The minimum weighted fraction of the sum total of weights\n",
    "    max_features='auto', # The number of features to consider when looking for the best split.\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0, # Threshold for early stopping in tree growth. \n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,  # The number of jobs to run in parallel. -1 means using all processors.\n",
    "    random_state=0, # \n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight='balanced')\n",
    "\n",
    "print('average value f1_macro (5 runs): ')\n",
    "print(np.mean(cross_val_score(clf, X_train, y_train, cv=5, scoring='f1_macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score for Random Forest is 0.77 which is relatively good.\n",
    "Let's get to another classifier, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6994059301001274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#define parameters for the model\n",
    "neighbors=50\n",
    "weights = 'uniform'\n",
    "p = 2\n",
    "\n",
    "cv_scores_train = []\n",
    "cv_scores_test = []\n",
    "\n",
    "# # in the loop, iterate over the number of neighbors and calculate the average value of F1\n",
    "for k in range(1, neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, algorithm='auto', leaf_size=30, p=2)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "   # calculating the average value of F1 for different partitions\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "    # calculating the average scores for all partitions\n",
    "    #print(np.mean(scores))\n",
    "    cv_scores_train.append(np.mean(scores))\n",
    "    \n",
    "    scores = cross_val_score(knn, X_test, y_test, cv=5, scoring='f1_macro')\n",
    "    # adding the next average value of f1_macro to the list\n",
    "    cv_scores_test.append(np.mean(scores))\n",
    "    \n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score for KNN is 0.69 which is lower than for the previous one. Guess I am opting for Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting scores for channels\n",
    "\n",
    "Now, I can proceed to work with CNN and BBC data using the model I've just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size (3929, 3)\n",
      "row number:  3929\n",
      "feature number:  4\n",
      "\n",
      "names of features:  ['id', 'date', 'text', 'channel']\n",
      "-------------------\n",
      "full data loaded\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "#firts, the dataset for BBC\n",
    "d1=pd.read_csv('bbchealth.txt',  sep='|', encoding='ANSI', header = None)\n",
    "\n",
    "print('Dataset size', d1.shape)\n",
    "\n",
    "\n",
    "d1 = d1.rename(columns={d1.columns[0]: \"id\", d1.columns[1]: \"date\", d1.columns[2]: \"text\"})\n",
    "#here, I add a new column with the name of the channel\n",
    "d1['channel'] = 'BBC'\n",
    "num_rows1, num_feature1 = d1.shape\n",
    "\n",
    "print('row number: ', num_rows1)\n",
    "print('feature number: ', num_feature1)\n",
    "print()\n",
    "print('names of features: ', list(d1))\n",
    "\n",
    "print('-------------------')\n",
    "print('full data loaded')\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585978391360221184</td>\n",
       "      <td>Thu Apr 09 01:31:50 +0000 2015</td>\n",
       "      <td>Breast cancer risk test devised http://bbc.in/...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>585947808772960257</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>GP workload harming care - BMA poll http://bbc...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>585947807816650752</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>Short people's 'heart risk greater' http://bbc...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>585866060991078401</td>\n",
       "      <td>Wed Apr 08 18:05:28 +0000 2015</td>\n",
       "      <td>New approach against HIV 'promising' http://bb...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585794106170839041</td>\n",
       "      <td>Wed Apr 08 13:19:33 +0000 2015</td>\n",
       "      <td>Coalition 'undermined NHS' - doctors http://bb...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                            date  \\\n",
       "0  585978391360221184  Thu Apr 09 01:31:50 +0000 2015   \n",
       "1  585947808772960257  Wed Apr 08 23:30:18 +0000 2015   \n",
       "2  585947807816650752  Wed Apr 08 23:30:18 +0000 2015   \n",
       "3  585866060991078401  Wed Apr 08 18:05:28 +0000 2015   \n",
       "4  585794106170839041  Wed Apr 08 13:19:33 +0000 2015   \n",
       "\n",
       "                                                text channel  \n",
       "0  Breast cancer risk test devised http://bbc.in/...     BBC  \n",
       "1  GP workload harming care - BMA poll http://bbc...     BBC  \n",
       "2  Short people's 'heart risk greater' http://bbc...     BBC  \n",
       "3  New approach against HIV 'promising' http://bb...     BBC  \n",
       "4  Coalition 'undermined NHS' - doctors http://bb...     BBC  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size (4045, 3)\n",
      "row number:  4045\n",
      "feature number:  4\n",
      "\n",
      "names of features:  ['id', 'date', 'text', 'channel']\n",
      "-------------------\n",
      "full data loaded\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1422: expected 3 fields, saw 4\\nSkipping line 1461: expected 3 fields, saw 4\\nSkipping line 1468: expected 3 fields, saw 4\\nSkipping line 1507: expected 3 fields, saw 4\\nSkipping line 1547: expected 3 fields, saw 4\\nSkipping line 1728: expected 3 fields, saw 4\\nSkipping line 1738: expected 3 fields, saw 4\\nSkipping line 1742: expected 3 fields, saw 4\\nSkipping line 1847: expected 3 fields, saw 5\\nSkipping line 2082: expected 3 fields, saw 4\\nSkipping line 2125: expected 3 fields, saw 4\\nSkipping line 3860: expected 3 fields, saw 4\\n'\n"
     ]
    }
   ],
   "source": [
    "#now, the dataset for CNN\n",
    "d2=pd.read_csv('cnnhealth.txt',  sep='|', encoding='ANSI', header = None, error_bad_lines=False)\n",
    "\n",
    "print('Dataset size', d2.shape)\n",
    "\n",
    "\n",
    "d2 = d2.rename(columns={d2.columns[0]: \"id\", d2.columns[1]: \"date\", d2.columns[2]: \"text\"})\n",
    "#again, creating a column with channel's name\n",
    "d2['channel'] = 'CNN'\n",
    "num_rows2, num_feature2 = d2.shape\n",
    "\n",
    "print('row number: ', num_rows2)\n",
    "print('feature number: ', num_feature2)\n",
    "print()\n",
    "print('names of features: ', list(d2))\n",
    "\n",
    "print('-------------------')\n",
    "print('full data loaded')\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576880531301801984</td>\n",
       "      <td>Sat Mar 14 23:00:11 +0000 2015</td>\n",
       "      <td>An abundance of online info can turn us into e...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>576820122666471424</td>\n",
       "      <td>Sat Mar 14 19:00:08 +0000 2015</td>\n",
       "      <td>A plant-based diet that incorporates fish may ...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>576744652717461504</td>\n",
       "      <td>Sat Mar 14 14:00:15 +0000 2015</td>\n",
       "      <td>It doesn't take much to damage your hearing at...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576736754436304896</td>\n",
       "      <td>Sat Mar 14 13:28:52 +0000 2015</td>\n",
       "      <td>RT @CNN: Forever young? Discover this islandвЂ...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>576736614766010368</td>\n",
       "      <td>Sat Mar 14 13:28:18 +0000 2015</td>\n",
       "      <td>RT @CNN: Is post-traumatic stress disorder in ...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                            date  \\\n",
       "0  576880531301801984  Sat Mar 14 23:00:11 +0000 2015   \n",
       "1  576820122666471424  Sat Mar 14 19:00:08 +0000 2015   \n",
       "2  576744652717461504  Sat Mar 14 14:00:15 +0000 2015   \n",
       "3  576736754436304896  Sat Mar 14 13:28:52 +0000 2015   \n",
       "4  576736614766010368  Sat Mar 14 13:28:18 +0000 2015   \n",
       "\n",
       "                                                text channel  \n",
       "0  An abundance of online info can turn us into e...     CNN  \n",
       "1  A plant-based diet that incorporates fish may ...     CNN  \n",
       "2  It doesn't take much to damage your hearing at...     CNN  \n",
       "3  RT @CNN: Forever young? Discover this islandвЂ...     CNN  \n",
       "4  RT @CNN: Is post-traumatic stress disorder in ...     CNN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size (7974, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585978391360221184</td>\n",
       "      <td>Thu Apr 09 01:31:50 +0000 2015</td>\n",
       "      <td>Breast cancer risk test devised http://bbc.in/...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>585947808772960257</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>GP workload harming care - BMA poll http://bbc...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>585947807816650752</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>Short people's 'heart risk greater' http://bbc...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>585866060991078401</td>\n",
       "      <td>Wed Apr 08 18:05:28 +0000 2015</td>\n",
       "      <td>New approach against HIV 'promising' http://bb...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585794106170839041</td>\n",
       "      <td>Wed Apr 08 13:19:33 +0000 2015</td>\n",
       "      <td>Coalition 'undermined NHS' - doctors http://bb...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                            date  \\\n",
       "0  585978391360221184  Thu Apr 09 01:31:50 +0000 2015   \n",
       "1  585947808772960257  Wed Apr 08 23:30:18 +0000 2015   \n",
       "2  585947807816650752  Wed Apr 08 23:30:18 +0000 2015   \n",
       "3  585866060991078401  Wed Apr 08 18:05:28 +0000 2015   \n",
       "4  585794106170839041  Wed Apr 08 13:19:33 +0000 2015   \n",
       "\n",
       "                                                text channel  \n",
       "0  Breast cancer risk test devised http://bbc.in/...     BBC  \n",
       "1  GP workload harming care - BMA poll http://bbc...     BBC  \n",
       "2  Short people's 'heart risk greater' http://bbc...     BBC  \n",
       "3  New approach against HIV 'promising' http://bb...     BBC  \n",
       "4  Coalition 'undermined NHS' - doctors http://bb...     BBC  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here, I join two datasets\n",
    "df = d1.append(d2)\n",
    "\n",
    "print('Dataset size', df.shape)\n",
    "df.head(5)\n",
    "#df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585978391360221184</td>\n",
       "      <td>Thu Apr 09 01:31:50 +0000 2015</td>\n",
       "      <td>breast cancer risk test devised</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>585947808772960257</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>gp workload harming care - bma poll</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>585947807816650752</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>short people 's 'heart risk greater '</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>585866060991078401</td>\n",
       "      <td>Wed Apr 08 18:05:28 +0000 2015</td>\n",
       "      <td>new approach against hiv 'promising '</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585794106170839041</td>\n",
       "      <td>Wed Apr 08 13:19:33 +0000 2015</td>\n",
       "      <td>coalition 'undermined nhs ' - doctors</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                            date  \\\n",
       "0  585978391360221184  Thu Apr 09 01:31:50 +0000 2015   \n",
       "1  585947808772960257  Wed Apr 08 23:30:18 +0000 2015   \n",
       "2  585947807816650752  Wed Apr 08 23:30:18 +0000 2015   \n",
       "3  585866060991078401  Wed Apr 08 18:05:28 +0000 2015   \n",
       "4  585794106170839041  Wed Apr 08 13:19:33 +0000 2015   \n",
       "\n",
       "                                    text channel  \n",
       "0        breast cancer risk test devised     BBC  \n",
       "1    gp workload harming care - bma poll     BBC  \n",
       "2  short people 's 'heart risk greater '     BBC  \n",
       "3  new approach against hiv 'promising '     BBC  \n",
       "4  coalition 'undermined nhs ' - doctors     BBC  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning and tokenizing the data\n",
    "df_clean0 = clear_data(df)\n",
    "df_clean = tokenize_and_stem(df_clean0)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents number :  7974 feature number:  1693\n"
     ]
    }
   ],
   "source": [
    "#creating a vector of words based on the pre-trained on the Amazon data vectorizer\n",
    "X = vec.transform(df_clean['text'])\n",
    "chan_data = vec.transform(df_clean['text']).toarray()\n",
    "\n",
    "num_docs, num_feature = X.shape\n",
    "print('documents number : ', num_docs, 'feature number: ', num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#fitting Random Forest classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#predicting based on the channels' data \n",
    "y_predicted1 = clf.predict(chan_data)\n",
    "\n",
    "print(y_predicted1) #this is the vector with sentiment scores for channels' dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7974 entries, 0 to 4044\n",
      "Data columns (total 5 columns):\n",
      "id         7974 non-null int64\n",
      "date       7974 non-null object\n",
      "text       7974 non-null object\n",
      "channel    7974 non-null object\n",
      "score      7974 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 373.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#len(y_predicted1)\n",
    "df_clean['score'] = y_predicted1 #adding scores to the dataset\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20bd2cd2f28>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGz9JREFUeJzt3X+U1XW97/Hni2kEMxLFwasMHliGPzBr1DmYZR2OdgQ9t7AWnujWcejYwu7FVqyb3RDXyjQ1Tc1fmedioGAWh6uRHK4n5QheclUyoMgP0Rh11IlfIyhqKjn0vn/sz+AG9gz7C/OdPTCvx1p7zf6+v5/vd793y9WL74/9+SoiMDMzK1efSjdgZmb7FweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSa5B4ekKklPSZqflodJekLSWkn/JumgVO+blpvS+qFF+7gs1Z+TNDrvns3MrGPdccTxLWBN0fL1wM0RMRx4Dbgo1S8CXouIjwA3p3FIGgGMB04CxgA/lVTVDX2bmVkJuQaHpFrgH4GfpWUBZwH3pyEzgfPT+7FpmbT+7DR+LDA7IrZFxItAEzAyz77NzKxjH8h5/7cA/wvon5YHAq9HRFtabgEGp/eDgVcAIqJN0tY0fjDwh6J9Fm+zg6SJwESAQw455LQTTjiha7+JmdkBbtmyZa9GRM2exuUWHJL+K7ApIpZJGtVeLjE09rCus23eL0RMA6YB1NfXx9KlSzP3bGbWm0l6qZxxeR5xfAr4vKTzgH7AhykcgQyQ9IF01FELrEvjW4AhQIukDwCHAluK6u2KtzEzs26W2zWOiLgsImojYiiFi9sLI+IrwCJgXBrWADyY3s9Ly6T1C6MwA+M8YHy662oYMBxYklffZmbWubyvcZTyXWC2pKuBp4DpqT4duFdSE4UjjfEAEbFa0hzgGaANmBQR27u/bTMzA9CBOK26r3GY2d567733aGlp4d133610K7np168ftbW1VFdX71SXtCwi6ve0fSWOOMzMeqyWlhb69+/P0KFDKfwi4MASEWzevJmWlhaGDRu2V/vwlCNmZkXeffddBg4ceECGBoAkBg4cuE9HVA4OM7NdHKih0W5fv5+Dw8zMMnFwmJl1kQkTJnD//ffveWAOmpub+ehHP9otn+WL4x047TuzKt1Cj7Hshgsr3YKZ9SAODtujl686udIt9BjHfG9lpVuwHmTWrFnceOONSOJjH/sYVVVVLF68mB//+Mds2LCBH/3oR4wbN4633nqLsWPH8tprr/Hee+9x9dVXM3bsWJqbmzn33HM588wz+d3vfsfgwYN58MEHOfjggxk1ahSnn346ixYt4vXXX2f69Ol8+tOfZvv27UyZMoXHHnuMbdu2MWnSJC6++OJu/d4+VWVmthdWr17NNddcw8KFC3n66ae59dZbAVi/fj2PP/448+fPZ8qUKUDhdxNz587lySefZNGiRXz729+m/Td0a9euZdKkSaxevZoBAwbwwAMP7PiMtrY2lixZwi233MKVV14JwPTp0zn00ENpbGyksbGRu+66ixdffLFbv7uPOMzM9sLChQsZN24cRxxxBACHH344AOeffz59+vRhxIgRbNy4ESj8dmLq1KksXryYPn368Kc//WnHumHDhlFXVwfAaaedRnNz847P+OIXv7hb/ZFHHmHFihU7rqVs3bqVtWvXctxxx+X+nds5OMzM9kJElLyttW/fvjuNAbjvvvtobW1l2bJlVFdXM3To0B2/oygeX1VVxTvvvLPbvqqqqmhra9uxz9tvv53Ro3d+GGpx4OTNp6rMzPbC2WefzZw5c9i8eTMAW7Zs6XDs1q1bGTRoENXV1SxatIiXXipr9vKSRo8ezZ133sl7770HwB//+Ef+/Oc/7/X+9oaPOMzM9sJJJ53E5Zdfzt/93d9RVVXFKaec0uHYr3zlK3zuc5+jvr6euro69uVBc1//+tdpbm7m1FNPJSKoqanh17/+9V7vb294ksMO+Hbc983tf0OlW+gxfFfVgW/NmjWceOKJlW4jd6W+Z7mTHPpUlZmZZeLgMDOzTBwcZmaWiYPDzMwycXCYmVkmuQWHpH6Slkh6WtJqSVem+j2SXpS0PL3qUl2SbpPUJGmFpFOL9tUgaW16NeTVs5mZ7Vmev+PYBpwVEW9JqgYel/Qfad13ImLXuYfPBYan1+nAncDpkg4HrgDqgQCWSZoXEa/l2LuZGdD1t+aXM9t0VVUVJ598MhFBVVUVP/nJT/jkJz9Jc3MzJ554IscffzwRwSGHHMLdd9/N8ccfD8CSJUu49NJL2bhxI5I488wzue222/jgBz/Ypd8htyOOKHgrLVanV2c/GhkLzErb/QEYIOkoYDSwICK2pLBYAIzJq28zs0o7+OCDWb58OU8//TQ//OEPueyyy3asO/bYY3esa2ho4NprrwVg48aNXHDBBVx//fU899xzrFmzhjFjxvDmm292eX+5XuOQVCVpObCJwv/5P5FWXZNOR90sqX2ilsHAK0Wbt6RaR/VdP2uipKWSlra2tnb5dzEzq4Q33niDww47bI/r7rjjDhoaGjjjjDOAwuNhx40bx5FHHtnlPeU65UhEbAfqJA0A5kr6KHAZsAE4CJgGfBe4Cij1ENzopL7rZ01L+6O+vv7A+zm8mfUa77zzDnV1dbz77rusX7+ehQsX7lj3/PPPU1dXx5tvvsnbb7/NE08U/j2+atUqGhq65xJwt9xVFRGvA48BYyJifTodtQ24GxiZhrUAQ4o2qwXWdVI3MzsgtZ+qevbZZ/nNb37DhRdeuGOm3fZTVc8//zy33HILEydO7Pb+8ryrqiYdaSDpYOCzwLPpugUqzEd8PrAqbTIPuDDdXfUJYGtErAceBs6RdJikw4BzUs3M7IB3xhln8Oqrr1LqFPznP/95Fi9eDBQmXVy2bFm39JTnEcdRwCJJK4BGCtc45gP3SVoJrASOAK5O4x8CXgCagLuA/wEQEVuAH6R9NAJXpZqZ2QHv2WefZfv27QwcOHC3dY8//jjHHnssAJdccgkzZ87cceoK4Oc//zkbNmzo8p5yu8YRESuA3eYZjoizOhgfwKQO1s0AZnRpg2ZmZSjn9tmu1n6NAwoPbpo5cyZVVVXA+9c4IoKDDjqIn/3sZwAceeSRzJ49m0svvZRNmzbRp08fPvOZz+x4imBX8vM4zMx6mO3bt5esDx06dKcnBO7qjDPO4Le//W1ebe3gKUfMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJr4d18ysEy9fdXKX7u+Y760sa9yGDRuYPHkyjY2N9O3bl6FDh3LLLbdw/PHHc9ttt/HNb34TKPzwr76+ngkTJjBhwgQWLFjACy+8QN++fXn11Vepr6+nubm5S7+DjzjMzHqYiOALX/gCo0aN4vnnn+eZZ57h2muvZePGjQwaNIhbb72Vv/zlLyW3raqqYsaMfH8v7eAwM+thFi1aRHV1Nd/4xjd21Orq6hgyZAg1NTWcffbZzJw5s+S2kydP5uabb6atrS23/hwcZmY9zKpVqzjttNM6XD9lyhRuuummkr8wP+aYYzjzzDO59957c+vPwWFmtp8ZNmwYI0eO5Be/+EXJ9VOnTuWGG27gr3/9ay6f7+AwM+thypkiferUqVx//fUlw+EjH/kIdXV1zJkzJ5f+HBxmZj3MWWedxbZt27jrrrt21BobG3nppZd2LJ9wwgmMGDGC+fPnl9zH5Zdfzo033phLf74d18ysE+XePtuVJDF37lwmT57MddddR79+/Xbcjlvs8ssv55RTdnt6BVA4ajn11FN58sknu7w/B4eZWQ909NFHlzzVtGrVqh3vP/7xj+90quqee+7ZaeyvfvWrXHrzqSozM8vEwWFmZpk4OMzMdlF4kvWBa1+/X27BIamfpCWSnpa0WtKVqT5M0hOS1kr6N0kHpXrftNyU1g8t2tdlqf6cpNF59Wxm1q9fPzZv3nzAhkdEsHnzZvr167fX+8jz4vg24KyIeEtSNfC4pP8A/idwc0TMlvSvwEXAnenvaxHxEUnjgeuBL0kaAYwHTgKOBv5T0nERUfqhvGZm+6C2tpaWlhZaW1sr3Upu+vXrR21t7V5vn1twRCGu30qL1ekVwFnAf0v1mcD3KQTH2PQe4H7gJ5KU6rMjYhvwoqQmYCTw+7x6N7Peq7q6mmHDhlW6jR4t12sckqokLQc2AQuA54HXI6J99q0WYHB6Pxh4BSCt3woMLK6X2Kb4syZKWipp6YH8LwUzs0rLNTgiYntE1AG1FI4STiw1LP1VB+s6qu/6WdMioj4i6mtqava2ZTMz24NuuasqIl4HHgM+AQyQ1H6KrBZYl963AEMA0vpDgS3F9RLbmJlZN8vzrqoaSQPS+4OBzwJrgEXAuDSsAXgwvZ+XlknrF6brJPOA8emuq2HAcGBJXn2bmVnn8ryr6ihgpqQqCgE1JyLmS3oGmC3pauApYHoaPx24N1383kLhTioiYrWkOcAzQBswyXdUmZlVTp53Va0Adpt9KyJeoHC9Y9f6u8AFHezrGuCaru7RzMyy8y/HzcwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZllkltwSBoiaZGkNZJWS/pWqn9f0p8kLU+v84q2uUxSk6TnJI0uqo9JtSZJU/Lq2czM9iy3Z44DbcC3I+JJSf2BZZIWpHU3R8SNxYMljQDGAycBRwP/Kem4tPoO4B+AFqBR0ryIeCbH3s3MrAO5BUdErAfWp/dvSloDDO5kk7HA7IjYBrwoqQkYmdY1RcQLAJJmp7EODjOzCuiWaxyShgKnAE+k0iWSVkiaIemwVBsMvFK0WUuqdVTf9TMmSloqaWlra2sXfwMzM2uXe3BI+hDwADA5It4A7gSOBeooHJHc1D60xObRSX3nQsS0iKiPiPqampou6d3MzHaX5zUOJFVTCI37IuJXABGxsWj9XcD8tNgCDCnavBZYl953VDczs26W511VAqYDayLix0X1o4qGfQFYld7PA8ZL6itpGDAcWAI0AsMlDZN0EIUL6PPy6tvMzDqX5xHHp4B/BlZKWp5qU4EvS6qjcLqpGbgYICJWS5pD4aJ3GzApIrYDSLoEeBioAmZExOoc+zYzs07keVfV45S+PvFQJ9tcA1xTov5QZ9uZmVn38S/HzcwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZllUlZwSHq0nJqZmR34Op2rSlI/4IPAEemBS+1zT32YwuNdzcysl9nTJIcXA5MphMQy3g+ONyg8B9zMzHqZToMjIm4FbpX0zYi4vZt6MjOzHqysadUj4nZJnwSGFm8TEbNy6svMzHqosoJD0r0UnhO+HNieygE4OMzMeplyH+RUD4yIiMizGTMz6/nK/R3HKuC/5NmImZntH8oNjiOAZyQ9LGle+6uzDSQNkbRI0hpJqyV9K9UPl7RA0tr097BUl6TbJDVJWiHp1KJ9NaTxayU17O2XNTOzfVfuqarv78W+24BvR8STkvoDyyQtACYAj0bEdZKmAFOA7wLnAsPT63TgTuB0SYcDV1A4XRZpP/Mi4rW96MnMzPZRuXdV/b+sO46I9cD69P5NSWuAwcBYYFQaNhN4jEJwjAVmpesof5A0QNJRaeyCiNgCkMJnDPDLrD2Zmdm+K/euqjcp/Gsf4CCgGvhzRHy4zO2HAqcATwBHplAhItZLGpSGDQZeKdqsJdU6qu/6GROBiQDHHHNMOW2ZmdleKPeIo3/xsqTzgZHlbCvpQ8ADwOSIeENSh0NLfXQn9V17nAZMA6ivr/fdX2ZmOdmr2XEj4tfAWXsaJ6maQmjcFxG/SuWN6RQU6e+mVG8BhhRtXgus66RuZmYVUO6pqi8WLfbh/QvVnW0jYDqwJiJ+XLRqHtAAXJf+PlhUv0TSbAoXx7emU1kPA9e2330FnANcVk7fZmbW9cq9q+pzRe/bgGYKF7M78yngn4GVkpan2lQKgTFH0kXAy8AFad1DwHlAE/A28DWAiNgi6QdAYxp3VfuFcjMz637lXuP4WtYdR8TjlL4+AXB2ifEBTOpgXzOAGVl7MDOzrlfug5xqJc2VtEnSRkkPSKrNuzkzM+t5yr04fjeFaxBHU7gV9t9TzczMeplyg6MmIu6OiLb0ugeoybEvMzProcoNjlclfVVSVXp9FdicZ2NmZtYzlRsc/wL8E7CBwjQi40h3PZmZWe9S7u24PwAa2icWTBMP3kghUMzMrBcp94jjY8Wz0abfUZyST0tmZtaTlRscfYp+ud1+xFHu0YqZmR1Ayv0//5uA30m6n8JUI/8EXJNbV2Zm1mOV+8vxWZKWUpjYUMAXI+KZXDszM7MeqezTTSkoHBZmZr3cXk2rbmZmvZeDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWWSW3BImpGeGLiqqPZ9SX+StDy9zitad5mkJknPSRpdVB+Tak2SpuTVr5mZlSfPI457gDEl6jdHRF16PQQgaQQwHjgpbfPT9md/AHcA5wIjgC+nsWZmViG5TVQYEYslDS1z+FhgdkRsA16U1ASMTOuaIuIFAEmz01j/gt3MrEIqcY3jEkkr0qms9hl3BwOvFI1pSbWO6ruRNFHSUklLW1tb8+jbzMzo/uC4EzgWqKPwJMGbUl0lxkYn9d2LEdMioj4i6mtq/Dh0M7O8dOszNSJiY/t7SXcB89NiCzCkaGgtsC6976huZmYV0K1HHJKOKlr8AtB+x9U8YLykvpKGAcOBJUAjMFzSMEkHUbiAPq87ezYzs53ldsQh6ZfAKOAISS3AFcAoSXUUTjc1AxcDRMRqSXMoXPRuAyZFxPa0n0uAh4EqYEZErM6rZzMz27M876r6cony9E7GX0OJpwqmW3Yf6sLWzMxsH/iX42ZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMsktOCTNkLRJ0qqi2uGSFkham/4eluqSdJukJkkrJJ1atE1DGr9WUkNe/ZqZWXnyPOK4BxizS20K8GhEDAceTcsA5wLD02sicCcUgga4AjgdGAlc0R42ZmZWGbkFR0QsBrbsUh4LzEzvZwLnF9VnRcEfgAGSjgJGAwsiYktEvAYsYPcwMjOzbtTd1ziOjIj1AOnvoFQfDLxSNK4l1Tqq70bSRElLJS1tbW3t8sbNzKygp1wcV4ladFLfvRgxLSLqI6K+pqamS5szM7P3dXdwbEynoEh/N6V6CzCkaFwtsK6TupmZVUh3B8c8oP3OqAbgwaL6henuqk8AW9OprIeBcyQdli6Kn5NqZmZWIR/Ia8eSfgmMAo6Q1ELh7qjrgDmSLgJeBi5Iwx8CzgOagLeBrwFExBZJPwAa07irImLXC+5mZtaNcguOiPhyB6vOLjE2gEkd7GcGMKMLWzMzs33QUy6Om5nZfsLBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsk4oEh6RmSSslLZe0NNUOl7RA0tr097BUl6TbJDVJWiHp1Er0bGZmBZU84vj7iKiLiPq0PAV4NCKGA4+mZYBzgeHpNRG4s9s7NTOzHXrSqaqxwMz0fiZwflF9VhT8ARgg6ahKNGhmZpULjgAekbRM0sRUOzIi1gOkv4NSfTDwStG2Lam2E0kTJS2VtLS1tTXH1s3MercPVOhzPxUR6yQNAhZIeraTsSpRi90KEdOAaQD19fW7rTczs65RkSOOiFiX/m4C5gIjgY3tp6DS301peAswpGjzWmBd93VrZmbFuj04JB0iqX/7e+AcYBUwD2hIwxqAB9P7ecCF6e6qTwBb209pmZlZ96vEqaojgbmS2j//FxHxG0mNwBxJFwEvAxek8Q8B5wFNwNvA17q/ZTMza9ftwRERLwAfL1HfDJxdoh7ApG5ozczMytCTbsc1M7P9gIPDzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLJNKTTliZtYlXr7q5Eq30GMc872V3fI5PuIwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsE/+Ow2w/dNp3ZlW6hR5jbv9Kd9D7+IjDzMwycXCYmVkmDg4zM8tkvwkOSWMkPSepSdKUSvdjZtZb7RfBIakKuAM4FxgBfFnSiMp2ZWbWO+0XwQGMBJoi4oWI+AswGxhb4Z7MzHql/eV23MHAK0XLLcDpxQMkTQQmpsW3JD3XTb0d8P4GjgBerXQfPcIVqnQHtgv/91lk3//7/JtyBu0vwVHqf43YaSFiGjCte9rpXSQtjYj6SvdhVor/++x++8upqhZgSNFyLbCuQr2YmfVq+0twNALDJQ2TdBAwHphX4Z7MzHql/eJUVUS0SboEeBioAmZExOoKt9Wb+BSg9WT+77ObKSL2PMrMzCzZX05VmZlZD+HgMDOzTBwc1ilP9WI9kaQZkjZJWlXpXnojB4d1yFO9WA92DzCm0k30Vg4O64ynerEeKSIWA1sq3Udv5eCwzpSa6mVwhXoxsx7CwWGd2eNUL2bW+zg4rDOe6sXMduPgsM54qhcz242DwzoUEW1A+1Qva4A5nurFegJJvwR+DxwvqUXSRZXuqTfxlCNmZpaJjzjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4Osx5C0n7xKGczB4fZPpB0iKT/K+lpSaskfUnS30r6XaotkdRfUj9Jd0taKekpSX+ftp8g6f9I+nfgkVT7jqRGSSskXVnRL2hWgv+FY7ZvxgDrIuIfASQdCjwFfCkiGiV9GHgH+BZARJws6QTgEUnHpX2cAXwsIrZIOgcYTmFKewHzJH0mTSNu1iP4iMNs36wEPivpekmfBo4B1kdEI0BEvJGmbjkTuDfVngVeAtqDY0FEtD9b4pz0egp4EjiBQpCY9Rg+4jDbBxHxR0mnAecBP6RwuqnUPD6lpqhv9+ddxv0wIv5313Vp1rV8xGG2DyQdDbwdET8HbgQ+ARwt6W/T+v7povdi4CupdhyFI5PnSuzyYeBfJH0ojR0saVD+38SsfD7iMNs3JwM3SPor8B7w3ykcNdwu6WAK1zc+C/wU+FdJK4E2YEJEbJN2PhCJiEcknQj8Pq17C/gqsKmbvo/ZHnl2XDMzy8SnqszMLBMHh5mZZeLgMDOzTBwcZmaWiYPDzMwycXCYmVkmDg4zM8vk/wMplhHWv6ECOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "sns.countplot(x='score', hue='channel', data=df_clean) #plotting scores for each channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the graph above it can be seen that the majority of news produced by both channels are rather neutral ('0' score), as predicted by the model. Nevertheless, in case of positive sentiment ('1' score) CNN wins. So, I can assume that CNN produces more positive news. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
